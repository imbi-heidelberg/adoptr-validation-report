# Scenario V: single-arm design, medium effect size {#scenarioV}

## Details

In this scenario, again a point prior is analyzed. 
The null hypothesis is $\delta \leq 0$ and we assume an alternative effect size
of $\delta = 0.3$
Type one error rate should be protected at 2.5\% and the design's power should
be at least 80\%.
Differently than in the previous scenarios, we are assuming a single-arm
design in throughout this scenario.

```{r}
# data distribution and hypotheses
datadist   <- Normal(two_armed = FALSE)
H_0        <- PointMassPrior(.0, 1)
prior      <- PointMassPrior(.3, 1)

# define constraints
alpha      <- 0.025
min_power  <- 0.8
toer_cnstr <- Power(datadist, H_0)   <= alpha
pow_cnstr  <- Power(datadist, prior) >= min_power
```



## Variant V-1, sensitivity to integration order {#variantV_1}

In this variant, the sensitivity of the optimization with respect to the
integration order is investigated. 
We apply three different integration orders: 5, 8, and 11.

### Objective

Expected sample size under the alternative point mass prior $\delta = 0.3$
is minimized.
```{r}
ess <- ExpectedSampleSize(datadist, prior)
```


### Constraints

No additional constraints are considered in this variant.


### Initial Design

In order to vary the initial design, `rpact` is not used in this variant.
Instead, the following heuristical considerations are made. 
A fixed design for these parameters would require
`r ceiling(pwr::pwr.t.test(d = .3, sig.level = .025, power = .8, alternative = "greater")$n)` 
subjects per group. We use the half of this as initial values for the 
sample sizes. 
The initial stop for futility is at $c_1^f=0$, i.e., if the effect shows 
in the opponent direction to the alternative. 
The starting values for the efficacy stop and for $c_2$ is the $1-\alpha$-
quantile of the normal distribution.

```{r}
init_design <- function(order) {
    TwoStageDesign(
        n1 = ceiling(pwr::pwr.t.test(d = .3, 
                                     sig.level = .025, 
                                     power = .8, 
                                     alternative = "greater")$n) / 2,
        c1f = 0,
        c1e = qnorm( 1 - 0.025),
        n2 = ceiling(pwr::pwr.t.test(d = .3, 
                                     sig.level = .025, 
                                     power = .8, 
                                     alternative = "greater")$n) / 2,
        c2 = qnorm(1 - 0.025),
        order = order
)
}

```


### Optimization 

The optimal design is computed for three different integration orders: 5, 8,
and 11. 

```{r}
opt_design <- function(order) {
    minimize(
        ess,
        subject_to(
            toer_cnstr,
            pow_cnstr
        ),
        initial_design = init_design(order),
        opts = opts
    )
}

opt <- tibble(
  order  = c(5, 8, 11),
  design = lapply(c(5, 8, 11), function(x) opt_design(x))
)
```

### Test cases

Check if the optimization algorithm converged in all cases.
```{r}
opt %>% 
  transmute(
      order, 
      iterations = purrr::map_int(opt$design, 
                                  ~.$nloptr_return$iterations) ) %>%
  {print(.); .} %>% 
  {testthat::expect_true(all(.$iterations < opts$maxeval))}
```

Test the constraints on type one error rate and power by simulation and 
compare the results to the outcome of `evaluate()`.

```{r}
opt %>% 
  transmute(
      order, 
      toer      = map_dbl(design,
                           ~evaluate(Power(datadist, PointMassPrior(.0, 1)), .$design) ),
      toer_sim  = purrr::map(opt$design, 
                         ~sim_pr_reject(.[[1]], .0, datadist)$prob), 
      power     = map_dbl(design,
                          ~evaluate(Power(datadist, PointMassPrior(.3, 1)), .$design) ),
      power_sim = purrr::map(opt$design, 
                         ~sim_pr_reject(.[[1]], .3, datadist)$prob) ) %>% 
  unnest() %>% 
  {print(.); .} %>% {
  testthat::expect_true(all(.$toer      <= alpha * (1 + tol)))
  testthat::expect_true(all(.$toer_sim  <= alpha * (1 + tol)))
  testthat::expect_true(all(.$power     >= min_power * (1 - tol)))
  testthat::expect_true(all(.$power_sim >= min_power * (1 - tol))) }
```



## Variant V-2, utility maximization {#variantV_2}


### Objective

In this variant, a utility function consisting of expected sample size and
power is minimized.
The parameter $\lambda$ that is describing the ratio between expected
sample size and power is varied. 

```{r}
pow <- Power(datadist, prior)
ess <- ExpectedSampleSize(datadist, prior)

obj <- function(lambda) {
  composite({ess - lambda * pow})
}
```


### Constraints

The type one error rate is controlled at `r alpha` on the boundary of the 
null hypothesis. Hence, the previous inequality can still be used.
There is no constraint on power any more because power is part of the 
objective utility function.


### Initial Design

The previous initial design with order $5$ is applied.


### Optimization 

The optimal design is computed for two values of $\lambda$: 100 and 200.

```{r}
opt_utility <- tibble(
  lambda = c(100, 200)
) %>%
  mutate(
    design = purrr::map(lambda, ~minimize(
          obj(.),
          subject_to(
              toer_cnstr
          ),
          
          initial_design = init_design(5), 
          opts           = opts)) 
)

```


### Test cases

Firstly, it is checked whether the maximum number of iterations was not 
exceeded in both flavours.
```{r}
opt_utility %>% 
  transmute(
      lambda, 
      iterations = purrr::map_int(opt_utility$design, 
                                  ~.$nloptr_return$iterations) ) %>%
  {print(.); .} %>% 
  {testthat::expect_true(all(.$iterations < opts$maxeval))}
```

Type one error rate control is tested for both designs by simulation and by 
`adoptr`'s function `evaluate`.
In addition, it is tested if the design with larger $\lambda$ (i.e.,
stronger focus on power), shows the larger overall power.
```{r}
opt_utility %>% 
  transmute(
      lambda, 
      toer      = map_dbl(design,
                           ~evaluate(Power(datadist, PointMassPrior(.0, 1)), .$design) ),
      toer_sim  = purrr::map(opt_utility$design, 
                         ~sim_pr_reject(.[[1]], .0, datadist)$prob), 
      power     = map_dbl(design,
                          ~evaluate(Power(datadist, PointMassPrior(.3, 1)), .$design) ),
      power_sim = purrr::map(opt_utility$design, 
                         ~sim_pr_reject(.[[1]], .3, datadist)$prob) ) %>% 
  unnest() %>% 
  {print(.); .} %>% {
  testthat::expect_true(all(.$toer     <= alpha * (1 + tol)))
  testthat::expect_true(all(.$toer_sim <= alpha * (1 + tol)))
  testthat::expect_lte(.$power[1], .$power[2]) }
```



Finally, the three designs computed so far are plotted together to allow 
comparison.

```{r, echo=FALSE}
x1 <- seq(0, 3.5, by = .01)

tibble(
    type  = c(
        "Power constraint", 
        "Utility maximization with lambda = 100", 
        "Utility maximization with lambda = 200" ), 
    design = list(
        opt %>% 
            filter(order == 11) %>% 
            pull(design) %>% 
            .[[1]] %>% 
            .$design, 
        opt_utility %>% 
            filter(lambda == 100) %>% 
            pull(design) %>% 
            .[[1]] %>% 
            .$design, 
        opt_utility %>% 
            filter(lambda == 200) %>% 
            pull(design) %>% 
            .[[1]] %>% 
            .$design ) ) %>% 
    group_by(type) %>% 
    do(
        x1 = x1,
        n  = adoptr::n(.$design[[1]], x1),
        c2 = c2(.$design[[1]], x1),
        CP = evaluate(ConditionalPower(datadist, prior), .$design[[1]], x1) ) %>% 
    unnest() %>% 
    mutate(
        section = ifelse(
            is.finite(c2), 
            "continuation", 
            ifelse(c2 == -Inf, "efficacy", "futility") ) ) %>% 
    gather(variable, value, n, c2, CP) %>% 
    ggplot(aes(x1, value, color = type)) +
        geom_line(aes(group = interaction(section, type))) + 
        facet_wrap(~variable, scales = "free_y") +
        labs(y = "", x = expression(x[1])) +
        scale_color_discrete("") +
        theme_bw() +
        theme(
            panel.grid      = element_blank(),
            legend.position = "bottom" )
```


## Variant V-3, n1-penalty {#variantV_3}

In this case, the influence of the regularization term `N1()` is investigated.

### Objective

In this case, a mixed criterion consisting of expected sample size and
$n_1$ is minimized.
```{r}
N1 <- N1()

obj3 <- function(lambda) {
  composite({ess + lambda * N1})
}
```


### Constraints

The inequalities from variant V.1 can still be used.



### Initial Design

The previous initial design with order $5$ is applied.


### Optimization 

The optimal design is computed for two values of $\lambda$: 0.05 and 0.2.

```{r}
opt3_design <- function(lambda) {

    minimize(
        obj3(lambda),
        subject_to(
            toer_cnstr,
            pow_cnstr
        ),
        initial_design = init_design(5),
        opts = opts
    )
}

opt3 <- lapply(c(.05, .2), function(x) opt3_design(x))
```


### Test cases
Check if the optimization algorithm converged in all cases.
```{r}
iters <- sapply(opt3, function(x) x$nloptr_return$iterations)

print(iters)

testthat::expect_true(all(iters < opts$maxeval))
```


Check if the n1 regularizer of the design with higher $\lambda$ is lower.
```{r}
testthat::expect_lte(
    evaluate(N1, opt3[[2]]$design),
    evaluate(N1, opt3[[1]]$design)
)


testthat::expect_lte(
    evaluate(N1, opt3[[1]]$design),
    evaluate(N1, opt1[[1]]$design)
)
```

Finally the three designs computed so far are plotted together to allow 
comparison.

```{r, echo = FALSE}
z1 <- seq(0, 3, by = .01)

tibble(
    type  = c("No Penalization", "Penalize n1 with lambda = 0.05", "Penalize n1 with lambda = 0.2"), 
    design = list(opt1[[1]]$design, opt3[[1]]$design, opt3[[2]]$design)
) %>% 
    group_by(type) %>% 
    do(
        z1 = z1,
        n  = adoptr::n(.$design[[1]], z1),
        c2 = c2(.$design[[1]], z1)
    ) %>% 
    unnest() %>% 
    mutate(
        section = ifelse(
            is.finite(c2), 
            "continuation", 
            ifelse(c2 == -Inf, "efficacy", "futility")
        )
    ) %>% 
    gather(variable, value, n, c2) %>% 
    ggplot(aes(z1, value, color = type)) +
        geom_line(aes(group = interaction(section, type))) + 
        facet_wrap(~variable, scales = "free_y") +
        theme_bw() +
        scale_color_manual(
          values = c(rgb(0,74,111, maxColorValue = 255),
                                    rgb(0,159,227, maxColorValue = 255),
                                    rgb(230,121,0, maxColorValue = 255))) +
        theme(
            panel.grid = element_blank(),
            legend.position = "bottom"
        )
```



## Variant V-4, n2-penalty {#variantV_4}

In this case the average over $n_2$ is penalized by the predefined score
`AverageN2`.

### Objective

In this case, a mixed criterion consisting of expected sample size and
average of $n_2$ is minimized.
```{r}
avn2 <- AverageN2()

obj4 <- function(lambda) {
  composite({ess + lambda * avn2})
}
```


### Constraints

The inequalities from variant V.1 can still be used.



### Initial Design

The previous initial design with order $5$ is applied.


### Optimization 

The optimal design is computed for two values of $\lambda$: 0.01 and 0.1.

```{r}
opt4_design <- function(lambda) {
    minimize(
        obj4(lambda),
        subject_to(
            toer_cnstr,
            pow_cnstr
        ),
        initial_design = init_design(5),
        upper_boundary_design = get_upper_boundary_design(init_design(5), c2_buffer=3),
        opts = opts
    )
}

opt4 <- lapply(c(.01, .1), function(x) opt4_design(x))
```


### Test cases
Check if the optimization algorithm converged in all cases.
```{r}
iters <- sapply(opt4, function(x) x$nloptr_return$iterations)

print(iters)

testthat::expect_true(all(iters < opts$maxeval))
```


Check if the average $n_2$ regularizer of the design with 
higher $\lambda$ is lower.

```{r}
testthat::expect_lte(
    evaluate(avn2, opt4[[2]]$design),
    evaluate(avn2, opt4[[1]]$design)
)


testthat::expect_lte(
    evaluate(avn2, opt4[[1]]$design),
    evaluate(avn2, opt1[[1]]$design)
)
```

Finally the three designs computed so far are plotted together to allow 
comparison.

```{r, echo = FALSE}
z1 <- seq(0, 3, by = .01)

tibble(
    type  = c("No Penalization", "Penalize AverageN2 with lambda = 0.01", 
              "Penalize AverageN2 with lambda = 0.1"), 
    design = list(opt1[[1]]$design, opt4[[1]]$design, opt4[[2]]$design)
) %>% 
    group_by(type) %>% 
    do(
        z1 = z1,
        n  = adoptr::n(.$design[[1]], z1),
        c2 = c2(.$design[[1]], z1)
    ) %>% 
    unnest() %>% 
    mutate(
        section = ifelse(
            is.finite(c2), 
            "continuation", 
            ifelse(c2 == -Inf, "efficacy", "futility")
        )
    ) %>% 
    gather(variable, value, n, c2) %>% 
    ggplot(aes(z1, value, color = type)) +
        geom_line(aes(group = interaction(section, type))) + 
        facet_wrap(~variable, scales = "free_y") +
        theme_bw() +
        scale_color_manual(
          values = c(rgb(0,74,111, maxColorValue = 255),
                                    rgb(0,159,227, maxColorValue = 255),
                                    rgb(230,121,0, maxColorValue = 255))) +
        theme(
            panel.grid = element_blank(),
            legend.position = "bottom"
        )
```

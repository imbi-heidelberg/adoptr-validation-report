--- 
title: "Validation Report for **adoptr** package"
author: "Kevin Kunzmann, Maximilian Pilz & Nico Bruder"
date: "`r Sys.Date()`"
bibliography:
- book.bib
- packages.bib
description: This is an automatically generated validation report for the **adoptr** R
  package published via **bookdown**.
documentclass: book
link-citations: yes
site: bookdown::bookdown_site
biblio-style: apalike
github-repo: kkmann/adoptr-validation-report
output:
  
    bookdown::gitbook:
      css: style.css
      config:
        toc:
          collapse: section
          scroll_highlight: yes
          before: |
            <li><a href="./">Validation Report for adoptr Package</a></li>
          after: |
            <li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
        download: ["pdf"]
        edit: https://github.com/kkmann/adoptr-validation-report/edit/master/%s
        sharing:
          facebook: no
          github: yes
          
    bookdown::pdf_book:
      includes:
        in_header: preamble.tex
      latex_engine: xelatex
      citation_package: natbib
      keep_tex: yes
      #classoption: oneside 
---


# Introduction

This work is licensed under the [CC-BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/deed.en)



## Preliminaries

R package validation for regulatory environments can be a 
tedious endeavour.
The authors firmly believe that under the current regulation,
there is no such thing as a 'validated R package':
validation is by definition a process conducted by the *user*.
This validation report merely aims at facilitating 
validation of **[adoptr](https://github.com/kkmann/adoptr)** as 
much as possible.
No warranty whatsoever as to the correctness of **adoptr** nor the
completeness of the validation report are given by the authors.

We assume that the reader is familiar with the notation an theoretical
background of **adoptr**.
Otherwise, the following resources might be of help:

* **adoptr** online documentation at https://kkmann.github.io/adoptr/
* paper on the theoretical background of the core **adoptr** functionality [@variational]
* a general overview on adaptive designs is given in [@Bauer2015] 
* a more extensive treatment of the subject in [@Wassmer2016].



## Scope

**adoptr** itself already makes extensive use of unittesting to 
ensure correctness of all implemented functions. 
Yet, due to constraints on the build-time for an R package, 
the range of scenarios covered in the unittests of **adoptr** is 
rather limited.
Furthermore, the current R unittesting framework does not permit 
an easy generation of a human-readable report of the test cases
to ascertain coverage and test quality.

Therefore, **adoptr** splits testing in two parts: technical 
correctness is ensured via an extensive unittesting suit in **adoptr**
itself (aiming to maintain a 100% code coverage).
The validation report, however, runs through a wide range of possible
application scenarios and ensures plausibility of results as well
as consistency with existing methods wherever possible.
The report itself is implemented as a collection of Rmarkdown documents
allowing to show both the underlying code as well as the corresponding 
output in a human-readable format.

The online version of the report is dynamically re-generated on a 
weekly basis based on the respective 
most current version of **adoptr** on CRAN.
The latest result of these builds is available at 
https://kkmann.github.io/adoptr-validation-report/.
To ensure early warning in case of any test-case failures, 
formal tests are implemented using the **testthat** package 
[@R-testthat].
I.e., the combination of using a unittesting framework, a continuous 
integration, and continuous deployment service leads to an always 
up-to-date validation report (build on the current R release on Linux).
Any failure of the integrated formal tests will cause the build status
of the validation report to switch from 'passing' to 'failed' and
the respective maintainer will be notified immediately.



### Validating a local installation of adoptr

Note that, strictly speaking, the online version of the validation
report only provides evidence of the correctness on the respective 
Travis-CI cloud virtual machine infrastructure using the respective 
most recent release of R and the most recent versions of the 
dependencies available on CRAN.
In some instances it might therefore be desireable to conduct a
local validaton of **adoptr**.

To do so, one should install **adoptr** with the `INSTALL_opts` option
to include tests and invoke the test suit locally via
```r
install.packages("adoptr", INSTALL_opts = c("--install-tests"))
tools::testInstalledPackage("adoptr", types = c("examples", "tests"))
```
Upon passing the test suit successfully, the validation report
can be build locally.
To do so, first clone the entire source directory and switch
to the newly created folder
```bash
git clone https://github.com/kkmann/adoptr-validation-report.git
cd adoptr-validation-report
```
Make sure that all packages requied for building the report are
available, i.e., install all dependencies listed in the top-level
`DESCRIPTION` file, e.g.,
```r
install.packages(c(
    "adoptr", 
    "tidyverse", 
    "bookdown", 
    "rpact", 
    "testthat", 
    "pwr" ) )
```
The book can then be build using the terminal command
```bash
Rscript -e 'bookdown::render_book("index.Rmd", output_format = "all")'
```
or directly from R via 
```r
bookdown::render_book("index.Rmd", output_format = "all")
```
This produces a new folder `_book` with the html and pdf versions
of the report.



## Validation Scenarios



### [Scenario I: Large effect, point prior](#scenarioI)

This is the default scenario. 

* **Data distribution:** Two-armed trial with normally distributed test statistic
* **Prior:** $\delta\sim\textbf{1}_{\delta=0.4}$
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant I.1: Minimizing Expected Sample Size under the Alternative](#variantI_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.4\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. All three **adoptr** variants (two-stage, group-sequential, one-stage) 
        comply with constraints. Internally validated by testing vs. simulated 
        values of the power curve at respective points.
    3.  Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 
    4. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.
    5. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    6. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?


#### [Variant I.2: Minimizing Expected Sample Size under the Null Hypothesis](#variantI_2)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\color{red}{\delta=0.0}\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Validate constraint compliance by testing vs. simulated 
        values of the power curve at respective points.
    3. $n()$ of optimal design is monotonously increasing on continuation area.
    4. $ESS$ of optimal two-stage design is lower than $ESS$ of externally 
        computed group-sequential design using the [rpact](https://rpact.org/) package.
    5. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?



#### [Variant I.3: Condtional Power Constraint](#variantI_3)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.4\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. $CP := \color{red}{\boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4, X_1 = x_1\big] \geq 0.7}$ for all $x_1\in(c_1^f, c_1^e)$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Check $Power$ and $TOER$ constraints with simulation.
        Check $CP$ constraint on 25 different values of $x_1$ in 
        $[c_1^f, c_1^e]$
    3. Are the $CP$ values at the 25 test-pivots obtained from simulation the 
        same as the ones obtained by using numerical integration via 
        `adoptr::evaluate`?
    4. Is $ESS$ of optimal two-stage design with $CP$ constraint higher than 
        $ESS$ of optimal two-stage design without this constraint?
        
        




### [Scenario II: Large effect, Gaussian prior](#scenarioII)

Similar scope to Scenario I, but with a continuous Gaussian prior on $\delta$.


* **Data distribution:** Two-armed trial with normally distributed test statistic
* **Prior:** $\delta\sim\mathcal{N}(0.4, .3)$
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant II.1: Minimizing Expected Sample Size](#variantII_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta> 0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. All designs comply with type one error rate constraints (tested via
      simulation).
    3. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.



#### [Variant II.2: Minimizing Expected Sample Size under the Null hypothesis](#variantII_2)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\color{red}{\delta\leq 0}\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta> 0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Does the design comply with $TOER$ constraint (via simulation)?
    3. Is $ESS$ lower than expected sample size under the null hypothesis 
      for the optimal two stage design from Variant II-1?
      
      


#### [Variant II.3: Condtional Power Constraint](#variantII_3)
* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta>0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. $CP := \color{red}{\boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta> 0.0, X_1 = x_1\big] \geq 0.7}$
       for all $x_1\in(c_1^f, c_1^e)$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Check $TOER$ constraint with simulation.
    3. Check $CP$ constraint on three different values of $x_1$ in 
        $(c_1^f, c_1^e)$
    4. Is $ESS$ of optimal two-stage design with $CP$ constraint higher than 
        $ESS$ of optimal two-stage design without the constraint?


      
      
      

### [Scenario III: Large effect, uniform prior](#scenarioIII)

* **Data distribution:** Two-armed trial with normally distributed test statistic
* **Prior:** sequence of uniform distributions
$\delta\sim\operatorname{Unif}(0.4 - \Delta_i, 0.4 + \Delta_i)$
around $0.4$ with $\Delta_i=(3 - i)/10$ for $i=0\ldots 3$. 
I.e., for $\Delta_3=0$ reduces to a point prior on $\delta=0.4$. 
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant III.1: Convergence under Prior Concentration](#variantIII_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta>0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Simulated type one error rate is compared to $TOER$ constraint for each
      design.
    3. $ESS$ decreases with prior variance.
    
Additionally, the designs are compared graphically. 
Inspect the plot to see convergence pattern.





### [Scenario IV: Smaller effect size, larger trials](#scenarioIV)



#### [Variant IV.1: Minimizing Expected Sample Size under the Alternative](#variantIV_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.2\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.2\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. All three adoptr variants (two-stage, group-sequential, one-stage) 
        comply with constraints. Internally validated by testing vs. simulated 
        values of the power curve at respective points.
    3. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.
    4. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    5. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?
    6. Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 



#### [Variant IV.2: Increasing Power](#variantIV_2)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.2\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.2\big] \geq \color{red}{0.9}$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Does the design respect all constraints (via simulation)?
    3. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.
    4. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    5. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?
    6. Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 




#### [Variant IV.3: Increasing Maximal Type One Error Rate](#variantIV_3)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.2\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.2\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq \color{red}{0.05}$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Does the design respect all constraints (via simulation)?
    3. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.
    4. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    5. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?
    6. Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 




### [Scenario V: Single-arm design, medium effect size](#scenarioV)

* **Data distribution:** \textcolor{red}{One-armed} trial with normally distributed test statistic
* **Prior:** $\delta\sim\delta_{0.3}$
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant V.1: Sensitivity to Integration Order](#variantV_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\color{red}{\delta=0.3}\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: integration order 5, 8, 11 two-stage designs. 
* **Formal tests:**
    1. Do all designs converge within the respective iteration limit?
    2. Do all designs respect all constraints (via simulation)?

    
#### [Variant V.2: Utility Maximization](#variantV_2)

* **Objective:** $\lambda\, Power - ESS := \lambda\,  \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.3\big] - \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big].$
  for $\lambda = 100$ and $200$
* **Constraints:** 
    1. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Do both designs respect the type one error rate constraint (via simulation)?
    3. Is the power of the design with larger $\lambda$ larger?
    
    
#### [Variant V.3: $n_1$ penalty](#variantV_3)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big] + \lambda \, n_1$ 
     for $\lambda = 0.05$ and $0.2$.
* **Constraints:** 
    1. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    2. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.3\big] \geq 0.8$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Do both designs respect the type one error rate
      and power constraints (via simulation)?
    3. Is $n_1$ for the optimal design smaller than the order-5 design in V.1?


#### [Variant V.4: $n_2$ penalty](#variantV_4)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big] + \lambda$ `AverageN2`
     for $\lambda = 0.01$ and $0.1$.
* **Constraints:** 
    1. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    2. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.3\big] \geq 0.8$
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. Do both designs respect the type one error rate and power
      constraints (via simulation)?
    3. Is the `AverageN2` for the optimal design smaller than for the order-5 
    design in V.1?




### [Scenario VI: Binomial distribution](#scenarioVI)

This scenario investigates the implementation of the binomial distribution.

* **Data distribution:** Two-armed trial with binomial distributed outcomes.
Thus $\delta := p_E - p_C$ refers to the rate difference here.
The control rate is assumed to equal $p_C = 0.3$.
* **Prior:** $\delta\sim\textbf{1}_{\delta=0.2}$
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant VI.1: Minimizing Expected Sample Size under the Alternative](#variantVI_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.2\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.2\big] \geq 0.9$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Number of iterations are checked against default maximum to ensure proper
      convergence.
    2. All three **adoptr** variants (two-stage, group-sequential, one-stage) 
        comply with constraints. Internally validated by testing vs. simulated 
        values of the power curve at respective points.
    3. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.
    4. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    5. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?



## Technical Setup

All scenarios are run in a single, shared R session.
Required packages are loaded here,
the random seed is defined and set centrally, and the default number
of iteration is increased to make sure that all scenarios 
converge properly.
Additionally R scripts with convenience functions are sourced here as well.
There are three additional functions for this report.
`rpact_design` creates a two-stage design via the package **rpact** [@R-rpact]
in the notation of **adoptr**.
`sim_pr_reject` and `sim_n` allow to simulate rejection probabilities
and expected sample sizes respectively by the **adoptr** routine `simulate`.
Furthermore, global tolerances for the validation are set. 
For error rates, a relative deviation of $1\%$ from the target value is 
accepted. 
(Expected) Sample sizes deviations are more liberally accepted up to an 
absolute deviation of $0.5$. 
```{r setup, results='hide'}
library(adoptr)
library(tidyverse)
library(rpact)
library(pwr)
library(testthat)
library(tinytex)

# load custom functions in folder subfolder '/R'
for (nm in list.files("R", pattern = "\\.[RrSsQq]$"))
   source(file.path("R", nm))

# define seed value
seed  <- 42

# define absolute tolerance for error rates
tol   <- 0.01

# define absolute tolerance for sample sizes
tol_n <- 0.5

# define custom tolerance and iteration limit for nloptr
opts = list(
    algorithm = "NLOPT_LN_COBYLA",
    xtol_rel  = 1e-5,
    maxeval   = 100000
)
```

<!--chapter:end:index.Rmd-->


# Scenario I: large effect, point prior {#scenarioI}

Placeholder


## Details
## Variant I-1: Minimizing Expected Sample Size under Point Prior {#variantI_1}
### Objective
### Constraints
### Initial Designs
### Optimization 
### Test Cases
## Variant I-2: Minimizing Expected Sample Size under Null Hypothesis {#variantI_2}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Variant I-3: Conditional Power Constraint {#variantI_3}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Plot Two-Stage Designs

<!--chapter:end:01-scenario-I.Rmd-->


# Scenario II: large effect, Gaussian prior {#scenarioII}

Placeholder


## Details
## Variant II-1: Minimizing Expected Sample Size under Point Prior {#variantII_1}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Variant II-2: Minimizing Expected Sample Size under Null Hypothesis {#variantII_2}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Variant II-3: Conditional Power Constraint {#variantII_3}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Plot Two-Stage Designs

<!--chapter:end:02-scenario-II.Rmd-->


# Scenario III: large effect, uniform prior {#scenarioIII}

Placeholder


## Details
## Variant III.1: Convergence under prior concentration {#variantIII_1}
### Objective
### Constraints
### Optimization problem
### Test cases
### Plot designs

<!--chapter:end:03-scenario-III.Rmd-->


# Scenario IV: smaller effect, point prior {#scenarioIV}

Placeholder


## Details
## Variant IV-1: Minimizing Expected Sample Size under Point Prior {#variantIV_1}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Variant IV-2: Increase Power {#variantIV_2}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Variant IV-3: Increase Type One Error rate {#variantIV_3}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test Cases
## Plot Two-Stage Designs

<!--chapter:end:04-scenario-IV.Rmd-->


# Scenario V: single-arm design, medium effect size {#scenarioV}

Placeholder


## Details
## Variant V-1, sensitivity to integration order {#variantV_1}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test cases
## Variant V-2, utility maximization {#variantV_2}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test cases
## Variant V-3, n1-penalty {#variantV_3}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test cases
## Variant V-4, n2-penalty {#variantV_4}
### Objective
### Constraints
### Initial Design
### Optimization 
### Test cases

<!--chapter:end:05-scenario-V.Rmd-->


# Scenario VI: binomial distribution, point prior {#scenarioVI}

Placeholder


## Details
## Variant VI-1: Minimizing Expected Sample Size Under Point Prior {#variantVI_1}
### Objective
### Constraints
### Initial Designs
### Optimization 
### Test Cases
## Variant VI-2: Minimizing Expected Sample Size under Null Hypothesis {#variantVI_2}
### Objective
### Constraints
### Initial design
### Optimization
### Test cases
## Variant VI-3: Conditional Power constraint
### Objective
### Constraints
### Initial Design
### Optimization
### Test Cases

<!--chapter:end:06-scenario-VI.Rmd-->


# Scenario VII: binomial distribution, Gaussian prior {#scenarioVII}

Placeholder


## Details
## Variant II-1: Minimizing Expected Sample Size under Point Prior {#variantVII-1}
### Objective
### Constraints
### Initial Designs
### Optimization
### Test Cases
## Variant VII-2: Minimizing Expected Sample Size under Null Hypothesis
### Objective
### Constraints
### Initial Design
### Optimization
### Test Cases
## Variant VII-3: conditional Power Constraint
### Objective
### Constraints
### Initial Design
### Optimization
### Test Cases
## Plot Two-Stage Designs

<!--chapter:end:07-scenario-VII.Rmd-->


# Scenario VIII: large effect, unknown variance {#scenarioVIII}

Placeholder


## Details
## Variant VIII-1: Minimizing Expected Sample Size under Point Prior {#variantVIII_1}
### Objective
### Constraints
### Initial Designs
### Optimization
### Test Cases
## Variant VIII-2: Comparison to Normal Distribution
### Generate Comparison Design
### Test Cases

<!--chapter:end:08-scenario-VIII.Rmd-->

# Scenario IX: time-to-event endpoints

## Details
In this scenario, we visit a different scenario: Instead of continuous or binary endpoints, we now consider time-to-event endpoints. Therefore, we conduct a log-rank test, for which `adoptr` provides the normal approximation of the test statistics. In order to obtain a one-parametric statistics, we need to fix the event probability $d$ during the trial. We assume that a hazard ratio $\theta>1$ implies favorable results. The null-hypothesis is given by $\mathcal{H}_0:\theta\leq 1$. In both of the following two scenarios, we assume a constant event rate of $d=0.7$ and the type one error should be bounded by $\alpha<2.5$ and a power of $0.8$ is necessary. 

```{r}
datadist <- Survival(0.7,two_armed = TRUE)
H_0 <- PointMassPrior(1.0,1)
alpha <- 0.025
min_power <- 0.8
```

## Variant IX-1: Minimizing expected sample size under point prior

Under the alternative, we us a hazard ratio of $\theta=1.4$ for the point prior of the alternative. 

```{r}
prior <- PointMassPrior(1.4,1)

toer_cnstr <- Power(datadist,H_0)<=alpha
pow_cnstr <- Power(datadist, prior)>=min_power
```

### Objective
The expected sample size under the alternative is minimized.
```{r}
ess <- ExpectedSampleSize(datadist,prior)
```

### Constraints
No additional constraints besides type one and type two error are considered in this variant. 

### Initial Design

The initial group-sequential and two-stage designs are chosen by using the `rpact` package, which calculates group-sequential designs. The one-stage design is chosen heuristically. 

```{r}
order <- 7L
# data frame of initial designs 
tbl_designs <- tibble(
    type    = c("one-stage", "group-sequential", "two-stage"),
    initial = list(
        OneStageDesign(150, 2.0),
        rpact_design(datadist, sig.level=0.025,power=0.8, two_armed=TRUE, order=order,hazardratio=1.4),
        TwoStageDesign(rpact_design(datadist, sig.level=0.025,power=0.8, two_armed=TRUE, 
                                    order=order,hazardratio=1.4))) )
```

The order of the integration is set to 7.

### Optimization

```{r}
tbl_designs <- tbl_designs %>% 
    mutate(
       optimal = purrr::map(initial, ~minimize(
         
          ess,
          subject_to(
              toer_cnstr,
              pow_cnstr
          ),
          
          initial_design = ., 
          opts           = opts)) )
```

### Test cases

We first check that the maximum number of iterations was not reached.

```{r}
tbl_designs %>% 
  transmute(
      type, 
      iterations = purrr::map_int(tbl_designs$optimal, 
                                  ~.$nloptr_return$iterations) ) %>%
  {print(.); .} %>% 
  {testthat::expect_true(all(.$iterations < opts$maxeval))}
```

Besides, we check whether the type one error was not exceeded and the necessary power is reached.

```{r}
tbl_designs %>% 
  transmute(
      type, 
      toer  = purrr::map(tbl_designs$optimal, 
                         ~sim_pr_reject(.[[1]], 1.0, datadist)$prob), 
      power = purrr::map(tbl_designs$optimal, 
                         ~sim_pr_reject(.[[1]], 1.4, datadist)$prob) ) %>% 
  unnest(., cols = c(toer, power)) %>% 
  {print(.); .} %>% {
  testthat::expect_true(all(.$toer  <= alpha * (1 + tol)))
  testthat::expect_true(all(.$power >= min_power * (1 - tol))) }
```

Since the degrees of freedom of the three design classes are ordered as
'two-stage' > 'group-sequential' > 'one-stage',
the expected sample sizes (under the alternative) should be ordered 
in reverse ('two-stage' smallest).
Additionally, expected sample sizes under both null and alternative
are computed both via `evaluate()` and simulation-based.

```{r}
ess0 <- ExpectedSampleSize(datadist, H_0)

tbl_designs %>% 
    mutate(
        ess      = map_dbl(optimal,
                           ~evaluate(ess, .$design) ),
        ess_sim  = map_dbl(optimal,
                           ~sim_n(.$design, 1.4, datadist)$n ),
        ess0     = map_dbl(optimal,
                           ~evaluate(ess0, .$design) ),
        ess0_sim = map_dbl(optimal,
                           ~sim_n(.$design, 1.0, datadist)$n ) ) %>% 
    {print(.); .} %>% {
    # sim/evaluate same under alternative?
    testthat::expect_equal(.$ess, .$ess_sim, 
                           tolerance = tol_n,
                           scale = 1)
    # sim/evaluate same under null?
    testthat::expect_equal(.$ess0, .$ess0_sim, 
                           tolerance = tol_n,
                           scale = 1)
    # monotonicity with respect to degrees of freedom
    testthat::expect_true(all(diff(.$ess) < 0)) }
```

The expected sample size under the alternative must be lower or equal than
the expected sample size of the initial `rpact` group-sequential design that
is based on the inverse normal combination test.

```{r}
testthat::expect_lte(
  evaluate(ess, 
             tbl_designs %>% 
                filter(type == "group-sequential") %>% 
                pull(optimal) %>% 
                .[[1]]  %>%
                .$design ),
    evaluate(ess, 
             tbl_designs %>% 
                filter(type == "group-sequential") %>% 
                pull(initial) %>% 
                .[[1]] ) )
```


<!--chapter:end:09-scenario-IX.Rmd-->


# Scenario X: Using further constraints

Placeholder


## General assumptions
## Constraint X-1: Maximal Sample Size
### Details
### Initial Designs
### Optimization
### Testcases

<!--chapter:end:10-scenario-X.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:11-references.Rmd-->


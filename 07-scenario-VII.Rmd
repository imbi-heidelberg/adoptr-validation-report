# Scenario VII: binomial distribution, Gaussian prior {#scenarioVII}

## Details

In this scenario, we revisit the case from [ScenarioVI](#variantVI), but are not assuming a point prior anymore.
We assume $p_C=0.3$ for the event rate in the control group. The parameter which can be varied is the event rate in the experimental group $p_E$ and we assume that the rate difference $p_E-p_C \sim \textbf{1}_{(-0.29,0.69)} \mathcal{N}(0.2,0.2)$.The restriction to the interval $(-0.29,0.69)$ is necessary to ensure that the parameter $p_E$ does not become smaller than $0$ or larger than $1$.

In order to fulfill regulatory considerations, the type one error rate is still protected under the point prior $\delta=0$ at the level of significance $\alpha=0.025$.

Since effect sizes less than a minimal clincally relevant effect do not show evidene against the null hypothesis, we assume a clinically relevant effect size $\delta =0.0$ and condition the prior on values $\delta > 0$ in order to compute the expected power. We assume a minimal expected power of at least $0.8$. 

```{r}
# data distribution and priors
datadist   <- Binomial(rate_control=0.3, two_armed = TRUE)
H_0        <- PointMassPrior(.0, 1)
prior      <- ContinuousPrior(function(x) 1/(pnorm(0.69,0.2,0.2)-pnorm(-0.29,0.2,0.2))*dnorm(x,0.2,0.2),
                              support = c(-0.29,0.69),
                              tighten_support = TRUE)

# define constraints on type one error rate and expected power
alpha      <- 0.025
min_epower <- 0.8
toer_cnstr <- Power(datadist, H_0) <= alpha
epow_cnstr <- Power(datadist, condition(prior, c(0.0,0.69))) >= min_epower
```

## Variant II-1: Minimizing Expected Sample Size under Point Prior

### Objective

Expected sample size under the full prior is minimized, i.e., $\boldsymbol{E}\big[n(\mathscr{D})\big]$.

```{r}
ess <- ExpectedSampleSize(datadist, prior)
```

### Constraints

No additional constraints are considered.

### Initial Designs

For this example, the optimal one-stage, group-sequential, and generic two-stage designs are computed. The initial design for the one-stage case is determined heuristically and both the group sequential and the generic two-stage designs are optimized starting from the corresponding group-sequential design as computed by the `rpact` package.

```{r}
order <- 7L
# data frame of initial designs 
tbl_designs <- tibble(
    type    = c("one-stage", "group-sequential", "two-stage"),
    initial = list(
        OneStageDesign(200, 2.0),
        rpact_design(datadist, 0.2, 0.025, 0.8, TRUE, order),
        TwoStageDesign(rpact_design(datadist, 0.2, 0.025, 0.8, TRUE, order))) )
```
The order of integration is set to 7.

### Optimization

```{r,eval=FALSE}
tbl_designs <- tbl_designs %>% 
    mutate(
       optimal = purrr::map(initial, ~minimize(
          ess,
          subject_to(
              toer_cnstr,
              epow_cnstr
          ),
          
          initial_design = ., 
          opts           = opts)) )
```

### Test Cases

To avoid improper solutions, it is first verified that the maximum number of iterations was not exceeded in any of the three cases. 

```{r}
tbl_designs %>% 
  transmute(
      type, 
      iterations = purrr::map_int(tbl_designs$optimal, 
                                  ~.$nloptr_return$iterations) ) %>%
  {print(.); .} %>% 
  {testthat::expect_true(all(.$iterations < opts$maxeval))}
```

Furthermore, the type one error rate constraint needs to be tested. 
```{r}
tbl_designs %>% 
  transmute(
      type, 
      toer = purrr::map(tbl_designs$optimal, 
                        ~sim_pr_reject(.[[1]], .0, datadist)$prob) ) %>% 
  unnest(., cols = c(toer)) %>% 
  {print(.); .} %>% {
  testthat::expect_true(all(.$toer <= alpha * (1 + tol))) }
```

The optimal two-stage design is more flexible than the other two designs, so the expected sample sizes under the prior should be ordered upwards as follows: optimal two-stage design < group sequential < one-stage. 

```{r}
essh0 <- ExpectedSampleSize(datadist, H_0)

tbl_designs %>% 
    mutate(
        ess       = map_dbl(optimal,
                            ~evaluate(ess, .$design) ),
        essh0     = map_dbl(optimal,
                            ~evaluate(essh0, .$design) ),
        essh0_sim = map_dbl(optimal,
                            ~sim_n(.$design, .0, datadist)$n ) ) %>% 
    {print(.); .} %>% {
    # sim/evaluate same under null?
    testthat::expect_equal(.$essh0, .$essh0_sim, 
                           tolerance = tol_n,
                           scale = 1)
    # monotonicity with respect to degrees of freedom
    testthat::expect_true(all(diff(.$ess) < 0)) }
```

## Variant II-2: Minimizing Expected Sample Size under Null Hypothesis

### Objective
In this scenario, the expected sample size conditioned on negative effect sizes is minimized, i.e.,

```{r}
ess_0 <- ExpectedSampleSize(datadist, condition(prior, c(-0.29,0)))
```

### Constraints
No additional constraints besides type one error rate and expected power are considered in this variant.

### Initial Design

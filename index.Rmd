--- 
title: "Validation Report for **adoptr** package"
author: "Kevin Kunzmann & Maximilian Pilz"
date: "`r Sys.Date()`"
bibliography:
- book.bib
- packages.bib
description: This is an automatically generated validation report for the **adoptr** R
  package published via **bookdown**.
documentclass: book
link-citations: yes
site: bookdown::bookdown_site
biblio-style: apalike
output:
    bookdown::gitbook:
      css: style.css
      config:
        toc:
          collapse: section
          before: |
            <li><a href="./">Validation Report for adoptr Package</a></li>
          after: |
            <li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
        download:
          ["pdf"]
        edit: 
          https://github.com/kkmann/adoptr-validation-report/edit/master/%s
        github-repo: 
          "kkmann/adoptr-validation-report"
    bookdown::pdf_book:
      includes:
        in_header: preamble.tex
      latex_engine: xelatex
      citation_package: natbib
      keep_tex: yes
---

# Introduction


## Concept

R package validation for regulatory environments is a tedious endevour.
Note that there is no such thing as a 'validated R package':
validation is by definition a process conducted by the *user*.
This valdation report may thus only be seen as a means to facilitate 
validation of **[adoptr](https://github.com/kkmann/adoptr)** as much as possible.
No warranty whatsoever as to the correctness of **adoptr** not the
completeness of the validation report are given by the authors!

We assume that the reader is familiar with the notation an theoretical
background of **adoptr**.
Otherwise, the resources linked at https://github.com/kkmann/adoptr might
be a good starting point *ToDo: publications*.
The report explores a variety of essential scenarios and both formally
tests results (wherever possible) using **testthat**.
Detailed results are also printed in the report itself.
Any failure of the integrated formal tests will cause the build status
of the validation report to switch from 'passing' to 'failed'.
An overview of the respective test scenarios is given in [Validation Scenarios].

The online version of this report can be found at 
https://kkmann.github.io/adoptr-validation-report/ and is automatically
rebuild and redeployed on a daily basis using Travis-CI.
It uses the respective most current CRAN version of **adoptr** (cf. below).
The source code repository of this report can be found at 
https://github.com/kkmann/adoptr-validation-report.



## Validation Scenarios



### [Scenario I: Large effect, point prior](#scenarioI)

This is the default scenario. 

* **Data distribution:** Two-armed trial with normally distributed test statistic
* **Prior:** $\delta\sim\delta_{0.4}$
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant I.1: Minimizing Expected Sample Size under the Alternative](#variantI_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.4\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. All three **adoptr** variants (two-stage, group-sequential, one-stage) 
        comply with constraints. Internally validated by testing vs. simulated 
        values of the power curve at respective points.
    2. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.
    3. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    4. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?
    5. Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 


#### [Variant I.2: Minimizing Expected Sample Size under the Null Hypothesis](#variantI_2)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\color{red}{\delta=0.0}\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Validate constraint compliance by testing vs. simulated 
        values of the power curve at respective points.
    2. $n()$ of optimal design is monotonously increasing on continuation area.
    3. $ESS$ of optimal two-stage design is lower than $ESS$ of externally 
        computed group-sequential design using the [rpact](https://rpact.org/) package.
    4. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?



#### [Variant I.3: Condtional Power Constraint](#variantI_3)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.4\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. $CP := \color{red}{\boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.4, X_1 = x_1\big] \geq 0.7}$ for all $x_1\in(c_1^f, c_1^e)$
* **Formal tests:**
    1. Check $Power$ and $TOER$ constraints with simulation.
        Check $CP$ constraint on three different values of $x_1$ in 
        $(c_1^f, c_1^e)$
    2. Are the $CP$ values at the three test-pivots obtained from simulation the 
        same as the ones obtained by using numerical integration via 
        `adoptr::evaluate`?
    3. Is $ESS$ of optimal two-stage design with $CP$ constraint higher than 
        $ESS$ of optimal two-stage design without this constraint?
        
        




### [Scenario II: Large effect, Gaussian prior](#scenarioII)

Similar in scope to Scenario I, but with a continuous Gaussian prior on $\delta$.


* **Data distribution:** Two-armed trial with normally distributed test statistic
* **Prior:** $\delta\sim\mathcal{N}(0.4, .3)$
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant II.1: Minimizing Expected Sample Size](#variantII_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta> 0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. All designs comply with type one error rate constraints (tested via
      simulation).
    2. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in turn lower than the one of the
        optimal one-stage design.



#### [Variant II.2: Minimizing Expected Sample Size under the Null hypothesis](#variantII_2)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\color{red}{\delta\leq 0}\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta> 0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Does the design comply with $TOER$ constraint (via simulation)?
    2. Check $CP$ constraint on three different values of $x_1$ in 
        $(c_1^f, c_1^e)$
    3. Is $ESS$ lower than expected sample size under the null hypothesis 
      for the optimal two stage design from Variant II-1?
      
      


#### [Variant II.3: Condtional Power Constraint](#variantII_3)
* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta>0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. $CP := \color{red}{\boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta> 0.0, X_1 = x_1\big] \geq 0.7}$
       for all $x_1\in(c_1^f, c_1^e)$
* **Formal tests:**
    1. Check $TOER$ constraint with simulation.
        Check $CP$ constraint on three different values of $x_1$ in 
        $(c_1^f, c_1^e)$
    2. Is $ESS$ of optimal two-stage design with $CP$ constraint higher than 
        $ESS$ of optimal two-stage design without the constraint?


      
      
      

### [Scenario III: Large effect, uniform prior](#scenarioIII)

* **Data distribution:** Two-armed trial with normally distributed test statistic
* **Prior:** sequence of uniform distributions
$\delta\sim\operatorname{Unif}(0.4 - \Delta_i, 0.4 + \Delta_i)$
around $0.4$ with $\Delta_i=(3 - i)/10$ for $i=0\ldots 3$. 
I.e., for $\Delta_3=0$ reduces to a point prior on $\delta=0.4$. 
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant III.1: Convergence under Prior Concentration](#variantIII_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta>0.0\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Simulated type one error rate is compared to $TOER$ constraint for each
      design.
    2. Number of iterations are checked agaist default maximum to ensure proper
      convergence.
    3. $ESS$ decreases with prior variance.
    
Additionally, the designs are compared graphically. 
Inspect the plot to see convergence pattern.





### [Scenario IV: Smaller effect size, larger trials](#scenarioIV)



#### [Variant IV.1: Minimizing Expected Sample Size under the Alternative](#variantIV_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.2\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.2\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. All three adoptr variants (two-stage, group-sequential, one-stage) 
        comply with costraints. Internally validated by testing vs. simulated 
        values of the power curve at respective points.
    2. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in tunr lower than the one of the
        optimal one-stage design.
    3. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    4. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?
    5. Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 



#### [Variant IV.2: Increasing Power](#variantIV_2)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.2\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.2\big] \geq \color{red}{0.9}$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Does the design respect all constraints (via simulation)?
    2. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in tunr lower than the one of the
        optimal one-stage design.
    3. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    4. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?
    5. Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 




#### [Variant IV.3: Increasing Maximal Type One Error Rate](#variantIV_3)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.2\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.2\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq \color{red}{0.05}$
    3. Three variants: two-stage, group-sequential, one-stage. 
* **Formal tests:**
    1. Does the design respect all constraints (via simulation)?
    2. $ESS$ of optimal two-stage design is lower than $ESS$ of optimal
        group-sequential one and that is in tunr lower than the one of the
        optimal one-stage design.
    3. $ESS$ of optimal group-sequential design is lower than $ESS$ of 
        externally computed group-sequential design using the [rpact](https://rpact.org/) package.
    4. Are the $ESS$ values obtained from simulation the same as the ones 
        obtained by using numerical integration via `adoptr::evaluate`?
    5. Is $n()$ of the optimal two-stage design monotonously decreasing on
        continuation area? 




### [Scenario V: Single-arm design, medium effect size](#scenarioV)

* **Data distribution:** \textcolor{red}{One-armed} trial with normally distributed test statistic
* **Prior:** $\delta\sim\delta_{0.3}$
* **Null hypothesis:** $\mathcal{H}_0:\delta \leq 0$


#### [Variant V.1: Sensitivity to Integration Order](#variantV_1)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big]$
* **Constraints:** 
    1. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\color{red}{\delta=0.3}\big] \geq 0.8$
    2. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    3. Three variants: integration order 5, 8, 11 two-stage designs. 
* **Formal tests:**
    1. Do all designs respect all constraints (via simulation)?
    2. Do all designs converge within the respective iteration limit?
    3. Does constraint compliance get better with increased order?
    4. Does the simulated $ESS$ get better with increased order?
    
    
#### [Variant V.2: Utility Maximization](#variantV_2)

* **Objective:** $\lambda\, Power - ESS := \lambda\,  \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.3\big] - \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big].$
  for $\lambda = 100$ and $200$
* **Constraints:** 
    1. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
* **Formal tests:**
    1. Do both desings respect the type one error rate constraint (via simulation)?
    2. Is the power of the design with larger $\lambda$ larger?
    
    
#### [Variant V.3: $n_1$ penalty](#variantV_3)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big] + \lambda \, n_1$ 
     for $\lambda = 0.05$ and $0.2$.
* **Constraints:** 
    1. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    2. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.3\big] \geq 0.8$
* **Formal tests:**
    1. Is $n_1$ for the optimal design smaller than the order-5 design in V.1?


#### [Variant V.4: $n_2$ penalty](#variantV_4)

* **Objective:** $ESS := \boldsymbol{E}\big[n(X_1)\,|\,\delta=0.3\big] + \lambda$ `AverageN2`
     for $\lambda = 0.01$ and $0.1$.
* **Constraints:** 
    1. $TOER := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.0\big] \leq 0.025$
    2. $Power := \boldsymbol{Pr}\big[c_2(X_1) < X_2\,|\,\delta=0.3\big] \geq 0.8$
* **Formal tests:**
    1. Is the `AverageN2` for the optimal design smaller than for the order-5 
    design in V.1?




## Technical Setup

All scenarios are run in a single, shared R session.
Required packages are loaded here *ToDo: make sure that is the case!*,
the random see is defined and set centrally, and the default number
of iteration is increased to make sure that all scenarios 
converge properly.
Additionally R scripts with convenience functions are sourced here as well *ToDo: descibe or eliminate these functions!*.
```{r setup}
library(adoptr)
library(tidyverse)

# load custom functions in folder subfolder '/R'
for (nm in list.files("R", pattern = "\\.[RrSsQq]$"))
   source(file.path("R", nm))

# define seed value
seed <- 42

# define custom tolerance and iteration limit for nloptr
opts = list(
    algorithm = "NLOPT_LN_COBYLA",
    xtol_rel  = 1e-5,
    maxeval   = 100000
)
```
